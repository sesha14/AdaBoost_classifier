{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results after training with decision tree of depth=1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       0.85      0.94      0.89        18\n",
      "           2       0.89      0.73      0.80        11\n",
      "\n",
      "    accuracy                           0.91        45\n",
      "   macro avg       0.91      0.89      0.90        45\n",
      "weighted avg       0.91      0.91      0.91        45\n",
      "\n",
      "[[16  0  0]\n",
      " [ 0 17  1]\n",
      " [ 0  3  8]]\n",
      "Accuracy: 0.91\n",
      "\n",
      "results after training with decision tree of depth=2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.67      0.80        18\n",
      "           2       0.65      1.00      0.79        11\n",
      "\n",
      "    accuracy                           0.87        45\n",
      "   macro avg       0.88      0.89      0.86        45\n",
      "weighted avg       0.91      0.87      0.87        45\n",
      "\n",
      "[[16  0  0]\n",
      " [ 0 12  6]\n",
      " [ 0  0 11]]\n",
      "Accuracy: 0.87\n",
      "\n",
      "results after training with decision tree of depth=3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "[[16  0  0]\n",
      " [ 0 17  1]\n",
      " [ 0  0 11]]\n",
      "Accuracy: 0.98\n",
      "\n",
      "results after training with decision tree of depth=4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "[[16  0  0]\n",
      " [ 0 17  1]\n",
      " [ 0  0 11]]\n",
      "Accuracy: 0.98\n",
      "\n",
      "results after training with decision tree of depth=5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "[[16  0  0]\n",
      " [ 0 17  1]\n",
      " [ 0  0 11]]\n",
      "Accuracy: 0.98\n",
      "\n",
      "results after training with decision tree of depth=6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "[[16  0  0]\n",
      " [ 0 17  1]\n",
      " [ 0  0 11]]\n",
      "Accuracy: 0.98\n",
      "6 dot files tree_depth.dot will be downloaded which contain the tree visualizations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Sesha Sai Sreevani Kappagantula\n",
    "# Load libraries\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "#load the iris dataset\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data[:, :], columns = iris.feature_names[:])\n",
    "y = pd.DataFrame(iris.target, columns =[\"Species\"])\n",
    "\n",
    "#split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "#train and predict using Decisiontree classifier for all 4 features with depth=1\n",
    "lr = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=1),n_estimators=50,\n",
    "                         learning_rate=1,\n",
    "                         random_state=0)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred=lr.predict(X_test)\n",
    "\n",
    "#metrics valuation\n",
    "print('results after training with decision tree of depth=1')\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Visualize the Tree\n",
    "# Creates dot file named tree.dot\n",
    "export_graphviz(\n",
    "            lr.estimators_[0],\n",
    "            out_file =  \"Tree_\" +\"depth1\"+\".dot\",\n",
    "            feature_names = list(X.columns),\n",
    "            class_names = iris.target_names,\n",
    "            filled = True,\n",
    "            rounded = True)\n",
    "\n",
    "#train and predict using Decisiontree classifier for all 4 features with depth=2\n",
    "lr = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=2),n_estimators=50,\n",
    "                         learning_rate=1,\n",
    "                         random_state=0)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred=lr.predict(X_test)\n",
    "\n",
    "#metrics valuation\n",
    "print('\\nresults after training with decision tree of depth=2')\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Visualize the Tree\n",
    "# Creates dot file named tree.dot\n",
    "export_graphviz(\n",
    "            lr.estimators_[0],\n",
    "            out_file =  \"Tree_\" +\"depth2\"+\".dot\",\n",
    "            feature_names = list(X.columns),\n",
    "            class_names = iris.target_names,\n",
    "            filled = True,\n",
    "            rounded = True)\n",
    "\n",
    "#train and predict using Decisiontree classifier for all 4 features with depth=3\n",
    "lr = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=3),n_estimators=50,\n",
    "                         learning_rate=1,\n",
    "                         random_state=0)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred=lr.predict(X_test)\n",
    "\n",
    "#metrics valuation\n",
    "print('\\nresults after training with decision tree of depth=3')\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Visualize the Tree\n",
    "# Creates dot file named tree.dot\n",
    "export_graphviz(\n",
    "            lr.estimators_[0],\n",
    "            out_file =  \"Tree_\" +\"depth3\"+\".dot\",\n",
    "            feature_names = list(X.columns),\n",
    "            class_names = iris.target_names,\n",
    "            filled = True,\n",
    "            rounded = True)\n",
    "\n",
    "#train and predict using Decisiontree classifier for all 4 features with depth=4\n",
    "lr = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=4),n_estimators=50,\n",
    "                         learning_rate=1,\n",
    "                         random_state=0)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred=lr.predict(X_test)\n",
    "\n",
    "#metrics valuation\n",
    "print('\\nresults after training with decision tree of depth=4')\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Visualize the Tree\n",
    "# Creates dot file named tree.dot\n",
    "export_graphviz(\n",
    "            lr.estimators_[0],\n",
    "            out_file =  \"Tree_\" +\"depth4\"+\".dot\",\n",
    "            feature_names = list(X.columns),\n",
    "            class_names = iris.target_names,\n",
    "            filled = True,\n",
    "            rounded = True)\n",
    "\n",
    "#train and predict using Decisiontree classifier for all 4 features with depth=5\n",
    "lr = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=5),n_estimators=50,\n",
    "                         learning_rate=1,\n",
    "                         random_state=0)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred=lr.predict(X_test)\n",
    "\n",
    "#metrics valuation\n",
    "print('\\nresults after training with decision tree of depth=5')\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Visualize the Tree\n",
    "# Creates dot file named tree.dot\n",
    "export_graphviz(\n",
    "            lr.estimators_[0],\n",
    "            out_file =  \"Tree_\" +\"depth5\"+\".dot\",\n",
    "            feature_names = list(X.columns),\n",
    "            class_names = iris.target_names,\n",
    "            filled = True,\n",
    "            rounded = True)\n",
    "\n",
    "#train and predict using Decisiontree classifier for all 4 features with depth=6\n",
    "lr = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=6),n_estimators=50,\n",
    "                         learning_rate=1,\n",
    "                         random_state=0)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred=lr.predict(X_test)\n",
    "\n",
    "#metrics valuation\n",
    "print('\\nresults after training with decision tree of depth=6')\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Visualize the Tree\n",
    "# Creates dot file named tree.dot\n",
    "export_graphviz(\n",
    "            lr.estimators_[0],\n",
    "            out_file =  \"Tree_\" +\"depth6\"+\".dot\",\n",
    "            feature_names = list(X.columns),\n",
    "            class_names = iris.target_names,\n",
    "            filled = True,\n",
    "            rounded = True)\n",
    "\n",
    "print(\"6 dot files tree_depth.dot will be downloaded which contain the tree visualizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: 6 dot files will be downloaded when the code is ran.\n",
    "#These contain the tree visualizations for 6 the cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Results of previous assignment(using Decision tree classifier)\n",
    "\n",
    "    Depths           Accuracy when gini      Accuracy when entropy\n",
    "\n",
    "    Depth 1:               0.8666                 0.8666\n",
    "    Depth 2:               0.8333                 0.8333\n",
    "    Depth 3:               0.8666                 0.9\n",
    "    Depth 4:               0.9333                 0.9\n",
    "    Depth 5:               0.9333                 0.9333\n",
    "    Depth 6:               0.9                    0.9333'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Results of this assignment (using AdaBoost classifier)\n",
    "\n",
    "    Depths         Accuracy\n",
    "    \n",
    "    Depth1:        0.91\n",
    "    Depth2:        0.87\n",
    "    Depth3:        0.98\n",
    "    Depth4:        0.98\n",
    "    Depth5:        0.98\n",
    "    Depth6:        0.98\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Conclusion:\n",
    "   --We observed that the accuracy at depth 1 itself AdaBoost classifier is giving accuracy higher than \n",
    "   Decision tree classifier used with depth1\n",
    "   \n",
    "   --We also observe that higher accuracy is obtained at lower depths very quickly by AdaBoost classifier than with\n",
    "   Decision tree classifier.At depth 3 itself it attained 0.98 accuracy which is very high and good\n",
    "   \n",
    "   --Furthermore, we do not see any overfitting scenario as in Decision tree.The accuracy is seen to be stable after \n",
    "   depth 3 than decreasing \n",
    "   \n",
    "   -However, on overall,AdaBoost clearly is giving higher accuracy than Decision tree in our assignment \n",
    "   even in its worst case \n",
    "   \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
